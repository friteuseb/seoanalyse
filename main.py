import subprocess
import sys
import os
import json
import logging
import argparse
from urllib.parse import urlparse
from termcolor import colored



def print_banner():
    print(colored("""
    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘            Visualisation SÃ©mantique Web v1.0                 â•‘
    â•‘            Analyse du Maillage Interne                       â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """, "cyan", attrs=["bold"]))

    print(colored("ğŸ“Š FONCTIONNALITÃ‰S", "yellow", attrs=["bold"]))
    print(colored("""
    1. Crawl du contenu des zones spÃ©cifiÃ©es
    2. Analyse des liens internes Ã©ditoriaux
    3. Analyse sÃ©mantique des thÃ©matiques
    4. Visualisation interactive des rÃ©sultats
    """, "white"))

    print(colored("ğŸ¯ ZONES D'ANALYSE", "yellow", attrs=["bold"]))
    print(colored("""
    Utilisez les sÃ©lecteurs CSS pour cibler prÃ©cisÃ©ment les zones :
    """, "white"))
    print(colored("    â€¢ ", "green") + "Classes : " + colored(".content, .article, .post", "cyan"))
    print(colored("    â€¢ ", "green") + "IDs : " + colored("#main-content, #article", "cyan"))
    print(colored("    â€¢ ", "green") + "Exclusions : " + colored("#content:not(.menu):not(.footer)", "cyan"))
    print()

def setup_argument_parser():
    parser = argparse.ArgumentParser(
        description=colored("Outil d'analyse sÃ©mantique et visualisation du maillage interne Ã©ditorial", "cyan", attrs=["bold"]),
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog=colored("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                   EXEMPLES D'UTILISATION                     â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
""", "yellow", attrs=["bold"]) + """
1. Analyser la page entiÃ¨re:
   """ + colored('python3 main.py https://example.com/', "cyan") + """

2. Analyser une zone par classe CSS:
   """ + colored('python3 main.py https://example.com/ ".content"', "cyan") + """

3. Analyser une zone par ID:
   """ + colored('python3 main.py https://example.com/ "#main-content"', "cyan") + """

4. Analyser plusieurs zones:
   """ + colored('python3 main.py https://example.com/ "#main-content, .article-content"', "cyan") + """

5. Analyser en excluant des zones:
   """ + colored('python3 main.py https://example.com/ "#main-content:not(.navigation)"', "cyan") + """
   """ + colored('python3 main.py https://example.com/ ".content:not(#menu):not(.sidebar)"', "cyan") + """

""" + colored("ğŸ“ NOTES:", "yellow", attrs=["bold"]) + """
""" + colored("â€¢", "green") + """ Sans sÃ©lecteur CSS spÃ©cifiÃ©, l'analyse portera sur toute la page
""" + colored("â€¢", "green") + """ Les sÃ©lecteurs CSS doivent Ãªtre entre guillemets
""" + colored("â€¢", "green") + """ Utilisez :not() pour exclure les zones non pertinentes
""" + colored("â€¢", "green") + """ L'analyse exclut automatiquement les liens externes
""")
    
    parser.add_argument("url", 
                       help=colored("URL du site Ã  analyser (ex: https://example.com/)", "cyan"))
    parser.add_argument("selector", 
                       nargs='?',  # Rend l'argument optionnel
                       default=None,  # Valeur par dÃ©faut si non spÃ©cifiÃ©
                       help=colored("SÃ©lecteur CSS pour cibler les zones Ã  analyser (optionnel)", "cyan"))
    return parser

# Configuration du logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')


def load_config():
    config_path = 'config.json'
    if not os.path.exists(config_path):
        logging.error(f"Error: {config_path} not found.")
        logging.info("Please copy 'config.json.example' to 'config.json' and fill in your API token.")
        sys.exit(1)

    with open(config_path) as f:
        config = json.load(f)
    return config

def validate_url(url):
    result = urlparse(url)
    return all([result.scheme, result.netloc])

def run_command(command):
    process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True, bufsize=1, universal_newlines=True)
    output = []
    for line in iter(process.stdout.readline, ''):
        print(line, end='')  # Affiche la ligne en temps rÃ©el
        output.append(line)  # Stocke la ligne pour un traitement ultÃ©rieur si nÃ©cessaire
    process.stdout.close()
    return_code = process.wait()
    if return_code:
        raise subprocess.CalledProcessError(return_code, command)
    return ''.join(output)

def run_crawl(url):
    try:
        output = run_command(['python3', '01_crawl.py', url])
        crawl_id_line = [line for line in output.split('\n') if line.startswith('Crawl terminÃ©. ID du crawl:')]
        if crawl_id_line:
            return crawl_id_line[0].split(': ')[1]
    except subprocess.CalledProcessError as e:
        logging.error(f"Crawl failed with error code {e.returncode}")
    return None

def run_internal_links_crawl(crawl_id, selector):
    try:
        run_command(['python3', '03_crawl_internal_links.py', crawl_id, selector])
    except subprocess.CalledProcessError as e:
        logging.error(f"Internal links crawl failed with error code {e.returncode}")

def run_analysis(crawl_id):
    try:
        run_command(['python3', '02_analyse.py', crawl_id])
    except subprocess.CalledProcessError as e:
        logging.error(f"Analysis failed with error code {e.returncode}")
  
def main(url=None, selector=None):
    parser = setup_argument_parser()
    
    # Si les arguments ne sont pas fournis directement, les prendre des args du parser
    if url is None:
        args = parser.parse_args()
        url = args.url
        selector = args.selector  # Peut Ãªtre None si non spÃ©cifiÃ©
    
    if not validate_url(url):
        logging.error(f"URL invalide: {url}")
        logging.info("L'URL doit commencer par http:// ou https://")
        return

    # Message adaptatif selon la prÃ©sence ou non du sÃ©lecteur
    if selector:
        logging.info(f"ğŸ¯ Analyse de {url} avec le sÃ©lecteur: {selector}")
    else:
        logging.info(f"ğŸ¯ Analyse complÃ¨te de {url}")

    config = load_config()
    if "HUGGINGFACEHUB_API_TOKEN" not in config:
        logging.error("âŒ Erreur: HUGGINGFACEHUB_API_TOKEN non trouvÃ© dans config.json")
        return

    os.environ["HUGGINGFACEHUB_API_TOKEN"] = config["HUGGINGFACEHUB_API_TOKEN"]

    logging.info("ğŸ”„ DÃ©marrage du crawl...")
    crawl_id = run_crawl(url)
    if not crawl_id:
        logging.error("âŒ Ã‰chec du crawl")
        return

    logging.info(f"âœ… Crawl terminÃ© avec l'ID: {crawl_id}")
    
    # Message adaptatif pour l'analyse des liens
    if selector:
        logging.info(f"ğŸ” Analyse des liens internes dans la zone sÃ©lectionnÃ©e...")
    else:
        logging.info(f"ğŸ” Analyse des liens internes sur toute la page...")
    
    run_internal_links_crawl(crawl_id, selector)

    logging.info("âœ… Analyse des liens terminÃ©e")
    logging.info("ğŸ§  DÃ©marrage de l'analyse sÃ©mantique...")
    run_analysis(crawl_id)

    # Message de fin avec information sur la portÃ©e de l'analyse
    scope = "la zone sÃ©lectionnÃ©e" if selector else "toute la page"
    logging.info(f"""
    âœ¨ Analyse terminÃ©e avec succÃ¨s !
    
    RÃ©sumÃ© :
    â€¢ URL analysÃ©e : {url}
    â€¢ PortÃ©e : {scope}
    â€¢ ID du crawl : {crawl_id}
    
    Pour visualiser les rÃ©sultats :
    1. Ouvrez votre navigateur
    2. AccÃ©dez au tableau de bord de visualisation
    3. SÃ©lectionnez le crawl avec l'ID: {crawl_id}
    """)

    
def cleanup():
    logging.info("ğŸ§¹ Nettoyage des ressources...")

if __name__ == "__main__":
    # Afficher la banniÃ¨re une seule fois au dÃ©but
    print_banner()
    
    # VÃ©rifier les arguments
    if len(sys.argv) < 3:
        parser = setup_argument_parser()
        parser.print_help()
        sys.exit(1)
    
    url = sys.argv[1]
    selector = sys.argv[2]
    
    try:
        main(url, selector)
    except KeyboardInterrupt:
        logging.info("\nâ›” Analyse interrompue par l'utilisateur")
        sys.exit(1)
    except Exception as e:
        logging.error(f"âŒ Une erreur inattendue est survenue: {e}")
    finally:
        cleanup()